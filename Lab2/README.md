# Lab 2: Fine-Tune a Transformer model for Language Transcription

## Intro
The purpose with this lab is to fine-tune a pre-trained model for automatic speech recognition (ASR), for any multilingual data. This lab features the pre-trained model Whisper (a checkpoint of the model) and uses the Common Voice dataset for fine-tuning the model for achieving stronger performance. The lab has the following tasks:

1. Fine-tune the Whispser transformer model for the Swedish
language
2. Build and run an inference pipeline with a Gradio UI on Hugging Face Spaces, for demonstrating the model
3. Discussing possible improvements to the model performance e.g. model-centric or data-centric approach.
4. (optional) Refactor the program into a feature engineering pipeline, training pipeline, and an inference program

## Tools
- Pre-trained Transformer model: Whisper
- Dataset: [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)
- Data preparation, training and fine-tuning: Google colab
- Inference UI: Hugging Face Spaces 
- *Model storage: Google Drive (or Hopsworks)

## Steps

## Improving pipeline scalability and model performance

## Speech transcription model
Link: https://huggingface.co/spaces/AbyelT/Swedish-language-transformer
